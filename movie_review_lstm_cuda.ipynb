{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"movie_review_lstm.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"qAeREfNXGU6S","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install torch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Tp1of10hEjFg","colab_type":"code","colab":{}},"cell_type":"code","source":[" #CUDA_LAUNCH_BLOCKING=1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h0uPb71m23L5","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch\n","from torch import nn\n","from string import punctuation\n","from collections import Counter\n","import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qeKhgJix52CN","colab_type":"code","colab":{}},"cell_type":"code","source":["###############################################################################\n","#######################  1. LOAD THE TRAINING TEXT  ###########################\n","###############################################################################\n","with open(\"reviews.txt\") as f:\n","    reviews = f.read()\n","    \n","with open(\"labels.txt\") as f:\n","    labels = f.read()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"92evhcb37tZ9","colab_type":"code","colab":{}},"cell_type":"code","source":["###############################################################################\n","##########################  2. TEXT PRE-PROCESSING  ###########################\n","###############################################################################\n","\n","def preprocess(text):\n","    text = text.lower() #소문자\n","    text= \"\".join([ch for ch in text if ch not in punctuation])\n","    #문장 부호를 제외하고, \"\"(아무것도 없다는 뜻)를 매 글자 사이에 넣어서 연결 - 문장 부호만 제거하고 그대로.\n","    #띄어쓰기 남아있음.\n","    all_reviews = text.split(\"\\n\") #줄바꿈으로 나눔.\n","    #text = \" \".join(text) #띄어쓰기 다시 만들어줌.\n","    all_words = text.split() #단어마다 쪼갬.\n","    \n","    return all_reviews, all_words\n","\n","\n","all_reviews, all_words = preprocess(reviews)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RN4eny3S9zAX","colab_type":"code","colab":{}},"cell_type":"code","source":["###############################################################################\n","##################  3. CREATE DICTIONARIES & ENCODE REVIEWS  ##################\n","###############################################################################\n","\n","word_counts = Counter(all_words) #list의 개수 세는 dic 생성, key에 개수\n","word_list = sorted(word_counts, reverse = True) #get하면 key값들을 얻음, 많은 순서대로 정렬 - list\n","vocab_to_int = {word:idx+1 for idx, word in enumerate(word_list)} #단어를 1,2,3,4,..로 번호 매긴 dic로.\n","int_to_vocab = {idx:word for word, idx in vocab_to_int.items()} #번호를 다시 단어로 dic.\n","encoded_reviews = [[vocab_to_int[word] for word in review.split()] for review in all_reviews] #review들을 단어의 번호들로 표현"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dcATtlSzFeG6","colab_type":"code","colab":{}},"cell_type":"code","source":["###############################################################################\n","#############################  4. ENCODE LABELS ###############################\n","###############################################################################\n","all_labels = labels.split(\"\\n\")\n","encoded_labels = [1 if label == \"positive\" else 0 for label in all_labels]\n","assert len(encoded_reviews) == len(encoded_labels), \"# of encoded reivews & encoded labels must be the same!\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Fftm8DH8GH7C","colab_type":"code","colab":{}},"cell_type":"code","source":["###############################################################################\n","#####################  5. GET RID OF LENGTH-0 REVIEWS   #######################\n","###############################################################################\n","\n","encoded_labels = np.array( [label for idx, label in enumerate(encoded_labels) if len(encoded_reviews[idx]) > 0] )\n","encoded_reviews = [review for review in encoded_reviews if len(review) > 0]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QG7_YShvK7pN","colab_type":"code","colab":{}},"cell_type":"code","source":["torch.Tensor([1,2,3]).to(\"cuda\") #이게 갑자기 안될 때가 있음"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GSMw1QAMIelU","colab_type":"code","colab":{}},"cell_type":"code","source":["###############################################################################\n","######################  6. MAKE ALL REVIEWS SAME LENGTH  #######################\n","###############################################################################\n","def pad_text(encoded_reviews, seq_length):\n","    \n","    reviews = []\n","    \n","    for review in encoded_reviews:\n","        if len(review) >= seq_length:\n","            reviews.append(review[:seq_length])\n","        else:\n","            reviews.append([0]*(seq_length-len(review)) + review)\n","        \n","    return np.array(reviews)\n","\n","\n","padded_reviews = pad_text(encoded_reviews, seq_length = 200)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6o7B3esoIid7","colab_type":"code","colab":{}},"cell_type":"code","source":["###############################################################################\n","##############  7. SPLIT DATA & GET (REVIEW, LABEL) DATALOADER  ###############\n","###############################################################################\n","train_ratio = 0.8\n","valid_ratio = (1 - train_ratio)/2 #2로 나누는 이유는? test set도 만들기 위해.\n","total = padded_reviews.shape[0] #25000\n","train_cutoff = int(total * train_ratio)\n","valid_cutoff = int(total * (1 - valid_ratio))\n","\n","train_x, train_y = padded_reviews[:train_cutoff], encoded_labels[:train_cutoff]\n","valid_x, valid_y = padded_reviews[train_cutoff : valid_cutoff], encoded_labels[train_cutoff : valid_cutoff]\n","test_x, test_y = padded_reviews[valid_cutoff:], encoded_labels[valid_cutoff:]\n","\n","\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","#이부분 안해주면 TensorDataset에서 int not callable 에러 발생\n","#longtensor(정수)로 만들어줘야함, 추후에 간단한 형태의 코드 생각해볼수도.\n","train_x, train_y=torch.Tensor(train_x).long(), torch.Tensor(train_y).long()\n","valid_x, valid_y=torch.Tensor(valid_x).long(), torch.Tensor(valid_y).long()\n","test_x, test_y=torch.Tensor(test_x).long(), torch.Tensor(test_y).long()\n","\n","\n","train_data = TensorDataset(train_x, train_y)\n","valid_data = TensorDataset(valid_x, valid_y)\n","test_data = TensorDataset(test_x, test_y)\n","\n","\n","batch_size = 50\n","train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n","valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True)\n","test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True)\n","\n","#dataloader의[0]은 batch size*문장 길이, [1]은 batch size*1(label이므로)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E1h5Efdm7Mdd","colab_type":"code","colab":{}},"cell_type":"code","source":["###############################################################################\n","#########################  8. DEFINE THE LSTM MODEL  ##########################\n","###############################################################################\n","class SentimentLSTM(nn.Module):\n","    \n","    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0.5):\n","        super().__init__()\n","        # params: \"n_\" means dimension\n","        self.n_vocab = n_vocab     # number of unique words in vocabulary\n","        self.n_layers = n_layers   # number of LSTM layers \n","        self.n_hidden = n_hidden   # number of hidden nodes in LSTM\n","        \n","        self.embedding = nn.Embedding(n_vocab, n_embed)\n","        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n","        self.dropout = nn.Dropout(drop_p)\n","        self.fc = nn.Linear(n_hidden, n_output)\n","        self.sigmoid = nn.Sigmoid()\n","        \n","        \n","    def forward (self, input_words):\n","                                             # INPUT   :  (batch_size, seq_length)\n","        batch_size=input_words.shape[0]\n","        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n","        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n","        lstm_out = self.dropout(lstm_out)\n","        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) # (batch_size*seq_length, n_hidden)\n","        fc_out = self.fc(lstm_out)                      # (batch_size*seq_length, n_output)\n","        sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n","        sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n","        \n","        # extract the output of ONLY the LAST output of the LAST element of the sequence\n","        sigmoid_last = sigmoid_out[:, -1]               # (batch_size, 1)\n","        \n","        return sigmoid_last, h\n","    \n","    \n","    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n","      #최초의 hidden state와 cell state를 0으로 하는것 같은데..\n","        \n","        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        weights = next(self.parameters()).data #parameter들을 불러옴.\n","        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device), #weights와 똑같은 data와 cuda 여부의 tensor 생성.\n","             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n","             \n","        return h"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VCs43tcAMgXf","colab_type":"code","colab":{}},"cell_type":"code","source":["###############################################################################\n","################  9. INSTANTIATE THE MODEL W/ HYPERPARAMETERS #################\n","###############################################################################\n","n_vocab = len(vocab_to_int)+1\n","n_embed = 400\n","n_hidden = 512\n","n_output = 1   # 1 (\"positive\") or 0 (\"negative\")\n","n_layers = 2\n","device = 'cuda' if torch.cuda.is_available else 'cpu'\n","\n","\n","net_cpu = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)\n","net=net_cpu.to(device)\n","\n","###############################################################################\n","#######################  10. DEFINE LOSS & OPTIMIZER  #########################\n","###############################################################################\n","from torch import optim\n","\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(net.parameters(), lr = 0.001)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4jzakuviCFuz","colab_type":"code","colab":{}},"cell_type":"code","source":["#net=net.to(device)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5MbEqr_kOSMu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":305},"outputId":"a34ba752-513a-43a3-8e01-278d1a4b518d","executionInfo":{"status":"ok","timestamp":1548317609420,"user_tz":-540,"elapsed":466773,"user":{"displayName":"Kyung Bok Lee","photoUrl":"","userId":"17865942328612250119"}}},"cell_type":"code","source":["###############################################################################\n","##########################  11. TRAIN THE NETWORK!  ###########################\n","###############################################################################\n","print_every = 100\n","step = 0\n","n_epochs = 4  # validation loss increases from ~ epoch 3 or 4\n","clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM/RNN\n","device = 'cuda' if torch.cuda.is_available else 'cpu'\n","\n","#net=net.to(device)\n","\n","for epoch in range(n_epochs):\n","    h = net.init_hidden(batch_size)\n","    \n","    for inputs, labels in train_loader:\n","        step += 1\n","        inputs, labels = inputs.cuda(), labels.cuda()\n","        \n","        # making requires_grad = False for the latest set of h\n","        h = tuple([each.data for each in h])   \n","        \n","        net.zero_grad()\n","        output, h = net(inputs)\n","        loss = criterion(output.squeeze(), labels.float())\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(net.parameters(), clip)\n","        optimizer.step()\n","        \n","        if (step % print_every) == 0:            \n","            ######################\n","            ##### VALIDATION #####\n","            ######################\n","            net.eval()\n","            valid_losses = []\n","            v_h = net.init_hidden(batch_size)\n","            \n","            for v_inputs, v_labels in valid_loader:\n","                v_inputs, v_labels = inputs.to(device), labels.to(device)\n","        \n","                v_h = tuple([each.data for each in v_h])\n","                \n","                v_output, v_h = net(v_inputs)\n","                v_loss = criterion(v_output.squeeze(), v_labels.float())\n","                valid_losses.append(v_loss.item())\n","\n","            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n","                  \"Step: {}\".format(step),\n","                  \"Training Loss: {:.4f}\".format(loss.item()),\n","                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n","            net.train()"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Epoch: 1/4 Step: 100 Training Loss: 0.6804 Validation Loss: 0.6719\n","Epoch: 1/4 Step: 200 Training Loss: 0.6822 Validation Loss: 0.6834\n","Epoch: 1/4 Step: 300 Training Loss: 0.5584 Validation Loss: 0.5176\n","Epoch: 1/4 Step: 400 Training Loss: 0.6567 Validation Loss: 0.6038\n","Epoch: 2/4 Step: 500 Training Loss: 0.4862 Validation Loss: 0.4441\n","Epoch: 2/4 Step: 600 Training Loss: 0.5540 Validation Loss: 0.4670\n","Epoch: 2/4 Step: 700 Training Loss: 0.3078 Validation Loss: 0.2559\n","Epoch: 2/4 Step: 800 Training Loss: 0.2578 Validation Loss: 0.2488\n","Epoch: 3/4 Step: 900 Training Loss: 0.3034 Validation Loss: 0.2673\n","Epoch: 3/4 Step: 1000 Training Loss: 0.2207 Validation Loss: 0.2246\n","Epoch: 3/4 Step: 1100 Training Loss: 0.2144 Validation Loss: 0.2092\n","Epoch: 3/4 Step: 1200 Training Loss: 0.3669 Validation Loss: 0.3383\n","Epoch: 4/4 Step: 1300 Training Loss: 0.1768 Validation Loss: 0.1301\n","Epoch: 4/4 Step: 1400 Training Loss: 0.1220 Validation Loss: 0.0969\n","Epoch: 4/4 Step: 1500 Training Loss: 0.0469 Validation Loss: 0.0284\n","Epoch: 4/4 Step: 1600 Training Loss: 0.1461 Validation Loss: 0.1216\n"],"name":"stdout"}]},{"metadata":{"id":"RQlgjO3f9LFd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"34d9de7d-9069-4401-a690-d17dddf51b53","executionInfo":{"status":"ok","timestamp":1548318197579,"user_tz":-540,"elapsed":2167,"user":{"displayName":"Kyung Bok Lee","photoUrl":"","userId":"17865942328612250119"}}},"cell_type":"code","source":["device"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"]},"metadata":{"tags":[]},"execution_count":25}]},{"metadata":{"id":"4uZV2dBQDgoW","colab_type":"code","colab":{}},"cell_type":"code","source":["asd=torch.Tensor([1,2]).to(device)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A1TXkOIxD1Jp","colab_type":"code","colab":{}},"cell_type":"code","source":["st=0\n","for inputs, labels in test_loader:\n","    st+=1\n","    inputs, labels=inputs.to(device), labels.to(device)\n","    if(st==1):\n","      asdf=inputs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UQl6TdvkESfg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"3a634eed-afcb-4f6a-a25c-30375a54ff4a","executionInfo":{"status":"ok","timestamp":1548320100038,"user_tz":-540,"elapsed":2141,"user":{"displayName":"Kyung Bok Lee","photoUrl":"","userId":"17865942328612250119"}}},"cell_type":"code","source":["asdf.float().type()"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'torch.cuda.FloatTensor'"]},"metadata":{"tags":[]},"execution_count":42}]},{"metadata":{"id":"dpJR2K1lFAE_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"ea17e079-2d7f-466a-c46c-612fece2ce05","executionInfo":{"status":"ok","timestamp":1548320396999,"user_tz":-540,"elapsed":6137,"user":{"displayName":"Kyung Bok Lee","photoUrl":"","userId":"17865942328612250119"}}},"cell_type":"code","source":["###############################################################################\n","################  12. TEST THE TRAINED MODEL ON THE TEST SET  #################\n","###############################################################################\n","net.eval()\n","test_losses = []\n","num_correct = 0\n","test_h = net.init_hidden(batch_size)\n","\n","for inputs, labels in test_loader:\n","    inputs, labels=inputs.to(device), labels.to(device)\n","    \n","    test_h = tuple([each.data for each in test_h])\n","    test_output, test_h = net(inputs)\n","    loss = criterion(test_output.float(), labels.float())\n","    test_losses.append(loss.item())\n","    \n","    preds = torch.round(test_output.squeeze())\n","    correct_tensor = preds.eq(labels.float().view_as(preds))\n","    correct = np.squeeze(correct_tensor.cpu().numpy())\n","    num_correct += np.sum(correct)\n","    \n","print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n","print(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))"],"execution_count":54,"outputs":[{"output_type":"stream","text":["Test Loss: 0.4961\n","Test Accuracy: 0.83\n"],"name":"stdout"}]},{"metadata":{"id":"Hkc3ZzyaOfQp","colab_type":"code","colab":{}},"cell_type":"code","source":["###############################################################################\n","############  13. TEST THE TRAINED MODEL ON A RANDOM SINGLE REVIEW ############\n","###############################################################################\n","def predict(net, review, seq_length = 200):\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    \n","    words = preprocess(review)\n","    assert False not in [word in vocab_to_int for word in words[1]], \"word not in dictionary\"\n","    encoded_words = [vocab_to_int[word] for word in words[1]]\n","    padded_words = pad_text([encoded_words], seq_length)\n","    padded_words = torch.from_numpy(padded_words).to(device)\n","    \n","    if(len(padded_words) == 0):\n","        \"Your review must contain at least 1 word!\"\n","        return None\n","    \n","    net.eval()\n","    h = net.init_hidden(1)\n","   # output, h = net(padded_words, h)\n","    output, h = net(padded_words)\n","    pred = torch.round(output.squeeze())\n","    msg = \"This is a positive review.\" if pred == 1 else \"This is a negative review.\"\n","    \n","    return msg\n","\n","\n","review1 = \"It made me cry.\"\n","review2 = \"It was so good it made me cry.\"\n","review3 = \"It's ok.\"\n","review4 = \"This movie had the best acting and the dialogue was so good. I loved it.\"\n","review5 = \"Garbage\"\n","                       ### OUTPUT ###\n","predict(net, review1)  ## negative ##\n","predict(net, review2)  ## positive ##\n","predict(net, review3)  ## negative ##\n","predict(net, review4)  ## positive ##\n","predict(net, review5)  ## negative ##\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"L1PaePJEFJLP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"141b750a-a1ef-4c35-d1bc-e7c349d81252","executionInfo":{"status":"ok","timestamp":1548324187602,"user_tz":-540,"elapsed":1805,"user":{"displayName":"Kyung Bok Lee","photoUrl":"","userId":"17865942328612250119"}}},"cell_type":"code","source":["predict(net, \"It was perfect\")"],"execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'This is a negative review.'"]},"metadata":{"tags":[]},"execution_count":85}]}]}